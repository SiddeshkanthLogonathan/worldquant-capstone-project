{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import yfinance\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = [\n",
    "    'AAPL', 'MSFT', 'GOOGL', 'AMZN', 'JNJ', 'JPM', 'V', 'PG', 'UNH', 'NVDA', \n",
    "    'DIS', 'HD', 'MA', 'PFE', 'BAC', 'KO', 'PEP', 'MRK', 'WMT', 'INTC', \n",
    "    'CSCO', 'T', 'VZ', 'CMCSA', 'CVX', 'XOM', 'IBM', 'MMM', 'NKE', 'ORCL', \n",
    "    'GE', 'WFC', 'MCD', 'C', 'TXN', 'BA', 'LLY', 'ABT', 'BMY', 'GS', \n",
    "    'SBUX', 'CAT', 'HON', 'AXP', 'MDT', 'DUK', 'RTX', 'LOW', 'UPS', 'CVS',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  50 of 50 completed\n"
     ]
    }
   ],
   "source": [
    "dataset = yfinance.download(stocks, end=\"2024-07-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[\"Adj Close\"]\n",
    "dataset = dataset.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Ticker          AAPL       ABT      AMZN       AXP        BA       BAC  \\\n",
       " Date                                                                     \n",
       " 2008-03-19  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       " 2008-03-20  0.027763  0.023350  0.043038  0.094762  0.018380  0.085581   \n",
       " 2008-03-24  0.046972 -0.004132  0.037710  0.031100  0.019920  0.014095   \n",
       " 2008-03-25  0.010392 -0.016237 -0.010270  0.002109 -0.005112 -0.034865   \n",
       " 2008-03-26  0.028940 -0.009169 -0.018225 -0.045254  0.005270 -0.027581   \n",
       " \n",
       " Ticker           BMY         C       CAT     CMCSA  ...      SBUX         T  \\\n",
       " Date                                                ...                       \n",
       " 2008-03-19  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
       " 2008-03-20  0.000000  0.102401  0.001220  0.002070  ...  0.001714  0.022191   \n",
       " 2008-03-24 -0.008854  0.034222  0.029938  0.043905  ...  0.020536  0.032022   \n",
       " 2008-03-25  0.015515  0.006446  0.008023  0.016329  ...  0.005589 -0.000789   \n",
       " 2008-03-26 -0.018519 -0.058497  0.009395 -0.040409  ... -0.018343 -0.006315   \n",
       " \n",
       " Ticker           TXN       UNH       UPS         V        VZ       WFC  \\\n",
       " Date                                                                     \n",
       " 2008-03-19  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       " 2008-03-20  0.006047 -0.006470  0.014439  0.138938  0.028182  0.064421   \n",
       " 2008-03-24  0.027935  0.011042  0.023723 -0.071795  0.023532 -0.014132   \n",
       " 2008-03-25  0.007568 -0.012042 -0.000273  0.058932 -0.001893 -0.000935   \n",
       " 2008-03-26 -0.011608 -0.033447 -0.008317  0.011226 -0.020596 -0.038990   \n",
       " \n",
       " Ticker           WMT       XOM  \n",
       " Date                            \n",
       " 2008-03-19  0.000000  0.000000  \n",
       " 2008-03-20  0.048247  0.006751  \n",
       " 2008-03-24  0.007514  0.011176  \n",
       " 2008-03-25 -0.010815 -0.008726  \n",
       " 2008-03-26 -0.002827  0.012442  \n",
       " \n",
       " [5 rows x 50 columns],\n",
       " Ticker          AAPL       ABT      AMZN       AXP        BA       BAC  \\\n",
       " Date                                                                     \n",
       " 2024-06-24  0.003133 -0.003973 -0.018564  0.004731  0.014386  0.013421   \n",
       " 2024-06-25  0.004468  0.001804  0.004149 -0.003802 -0.022334 -0.015992   \n",
       " 2024-06-26  0.019993 -0.005783  0.039015  0.000390  0.019417 -0.009650   \n",
       " 2024-06-27  0.003986 -0.002193  0.021900 -0.009884  0.022465  0.006410   \n",
       " 2024-06-28 -0.016254 -0.007071 -0.023250  0.013792 -0.002740  0.013248   \n",
       " \n",
       " Ticker           BMY         C       CAT     CMCSA  ...      SBUX         T  \\\n",
       " Date                                                ...                       \n",
       " 2024-06-24  0.007155  0.022504  0.006589  0.000000  ... -0.002253  0.013587   \n",
       " 2024-06-25 -0.000947  0.003913 -0.007879 -0.002079  ... -0.005644  0.002145   \n",
       " 2024-06-26  0.007348 -0.005359 -0.002046 -0.004687  ... -0.001640  0.003745   \n",
       " 2024-06-27 -0.023294  0.004898  0.002234  0.001570  ...  0.001137 -0.000533   \n",
       " 2024-06-28  0.000482  0.031032  0.017224  0.022989  ... -0.017542  0.019200   \n",
       " \n",
       " Ticker           TXN       UNH       UPS         V        VZ       WFC  \\\n",
       " Date                                                                     \n",
       " 2024-06-24 -0.008285  0.015313  0.014641  0.003924  0.023111  0.015663   \n",
       " 2024-06-25  0.002888 -0.011184 -0.030592 -0.010025 -0.002186 -0.030673   \n",
       " 2024-06-26 -0.007045 -0.000949  0.028208  0.000256 -0.001461 -0.003322   \n",
       " 2024-06-27  0.001709  0.004958 -0.016793 -0.025621 -0.004876  0.007192   \n",
       " 2024-06-28  0.005687  0.046912  0.007509 -0.015454  0.010289  0.034309   \n",
       " \n",
       " Ticker           WMT       XOM  \n",
       " Date                            \n",
       " 2024-06-24  0.014578  0.029704  \n",
       " 2024-06-25 -0.021480  0.002806  \n",
       " 2024-06-26  0.012904  0.000350  \n",
       " 2024-06-27 -0.006004  0.004283  \n",
       " 2024-06-28 -0.002504  0.001915  \n",
       " \n",
       " [5 rows x 50 columns])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_data = dataset.pct_change().fillna(0)\n",
    "return_data.head(), return_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data splitting. Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2868, 50), (1230, 50))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test = train_test_split(return_data, test_size=0.3, shuffle=False, random_state=7)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.groupby([X_train.index.isocalendar().year, X_train.index.isocalendar().week]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_idx = pd.to_datetime(['{}-{}0'.format(i, j) for i, j in X_train.index], format=\"%Y-%W%w\")\n",
    "X_train = X_train.set_index(new_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookback_window = 52 # last 1 year\n",
    "predict_period = 4 # predicting next 4 weeks of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinancialDataset(Dataset):\n",
    "    def __init__(self, dataset, L, T):\n",
    "        self.dataset = dataset\n",
    "        self.L = L\n",
    "        self.T = T\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset) - self.L - self.T\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X_batch = self.dataset[idx:idx+self.L, :]\n",
    "        y_batch = self.dataset[idx+self.L:idx+self.L+self.T, :]\n",
    "        return (torch.FloatTensor(X_batch), \n",
    "                torch.FloatTensor(y_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_iter_mp = DataLoader(dataset=FinancialDataset(dataset=X_train.values, L=lookback_window, T=predict_period), \n",
    "                          batch_size=BATCH_SIZE, \n",
    "                          shuffle=False)\n",
    "\n",
    "X_train_iter_sp = DataLoader(dataset=FinancialDataset(dataset=X_train.values, L=lookback_window, T=1), \n",
    "                          batch_size=BATCH_SIZE, \n",
    "                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 52, 50]) torch.Size([16, 4, 50])\n"
     ]
    }
   ],
   "source": [
    "for X in X_train_iter_mp:\n",
    "    print(X[0].shape, X[1].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPOCHS = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution: Use Neural Networks to approximate the function. \n",
    "The Loss function is the optimization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_STOCKS = len(stocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeToVecEmbedding(nn.Module):\n",
    "    def __init__(self, k):\n",
    "        super(TimeToVecEmbedding, self).__init__()\n",
    "        self.k = k\n",
    "        self.dense = nn.Linear(in_features=k, out_features=k, bias=True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.dense(X)\n",
    "        time_encoding = torch.sin(X[:, :, 1:]) # periodic activation function\n",
    "        time_encoding = torch.cat([X[:, :, 0].unsqueeze(2), time_encoding], dim=-1)\n",
    "        return time_encoding\n",
    "\n",
    "class TransformerNet(nn.Module):\n",
    "    def __init__(self, num_features, num_assets, num_head, num_layers, W, T):\n",
    "        super(TransformerNet, self).__init__()\n",
    "        self.W = W - T\n",
    "        self.T = T\n",
    "        self.num_features = num_features\n",
    "        self.num_assets = num_assets\n",
    "        self.t2v_enc = TimeToVecEmbedding(k=self.W)\n",
    "        self.t2v_dec = TimeToVecEmbedding(k=self.T)\n",
    "        self.transformer = nn.Transformer(d_model=num_features, \n",
    "                                          nhead=num_head, \n",
    "                                          num_encoder_layers=num_layers, \n",
    "                                          num_decoder_layers=num_layers,\n",
    "                                          batch_first=True)\n",
    "        self.fcl = nn.Linear(in_features=num_features*T, out_features=num_assets*T)\n",
    "        self.dropout = nn.Dropout(p=0.25)\n",
    "        self.norm = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X_ = X[:, :self.W, :]\n",
    "        y_ = X[:, -self.T:, :]\n",
    "        X_ = torch.permute(X_, (0,2,1))\n",
    "        y_ = torch.permute(y_, (0,2,1))\n",
    "        X_emb = self.t2v_enc(X_)\n",
    "        X_emb = torch.permute(X_emb, (0,2,1))\n",
    "        y_emb = self.t2v_dec(y_)\n",
    "        y_emb = torch.permute(y_emb, (0,2,1))\n",
    "        logits = self.transformer(src=X_emb, tgt=y_emb)\n",
    "        logits = logits.view(-1, self.T*self.num_assets)\n",
    "        logits = self.dropout(self.fcl(logits))\n",
    "        logits = logits.view(-1, self.T, self.num_assets)\n",
    "        logits = self.norm(logits)\n",
    "        return logits\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.12/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransformerNet(\n",
       "  (t2v_enc): TimeToVecEmbedding(\n",
       "    (dense): Linear(in_features=48, out_features=48, bias=True)\n",
       "  )\n",
       "  (t2v_dec): TimeToVecEmbedding(\n",
       "    (dense): Linear(in_features=4, out_features=4, bias=True)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-9): 10 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=50, out_features=50, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=50, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=50, bias=True)\n",
       "          (norm1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-9): 10 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=50, out_features=50, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=50, out_features=50, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=50, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=50, bias=True)\n",
       "          (norm1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (fcl): Linear(in_features=200, out_features=200, bias=True)\n",
       "  (dropout): Dropout(p=0.25, inplace=False)\n",
       "  (norm): Softmax(dim=2)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_model = TransformerNet(num_features=NUM_STOCKS, \n",
    "                                   num_assets=NUM_STOCKS,\n",
    "                                   num_head=25, \n",
    "                                   num_layers=10,\n",
    "                                   W=lookback_window,\n",
    "                                   T=predict_period)\n",
    "transformer_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepModel(pl.LightningModule):\n",
    "    def __init__(self, model, loss_function):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.loss_function = loss_function\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X.unsqueeze(0)\n",
    "        weights = self.model(X.float())\n",
    "        weights = weights.squeeze(0)\n",
    "        return weights\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-5)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        X, y = train_batch\n",
    "        trading_weights = self.model(X)\n",
    "        loss = self.loss_function(trading_weights, y)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=False)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, test_batch, batch_idx):\n",
    "        X, y = test_batch\n",
    "        trading_weights = self.model(X)\n",
    "        loss = self.loss_function(trading_weights, y)\n",
    "        self.log(\"test_loss\", loss, on_step=True, on_epoch=False, prog_bar=True, logger=False)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiPeriodOptimizationObjective(nn.Module):\n",
    "    def __init__(self, holding_cost=0, trading_cost=0.01, risk_aversion_coeff=0.5):\n",
    "        super(MultiPeriodOptimizationObjective, self).__init__()\n",
    "        self.holding_cost=holding_cost\n",
    "        self.trading_cost=trading_cost\n",
    "        self.risk_aversion_coeff=risk_aversion_coeff\n",
    "\n",
    "    def forward(self, weights_t, mean_returns):\n",
    "        portfolio_returns = (weights_t*mean_returns).sum(2)\n",
    "        portfolio_Sigma = self.batch_cov(mean_returns)\n",
    "        portfolio_variance = (weights_t@(portfolio_Sigma@torch.permute(weights_t, (0,2,1)))).sum(dim=2)\n",
    "        delta_weights = (weights_t[:, 1:, :] - weights_t[:, :-1, :]).abs()\n",
    "        portfolio_trade_cost = (self.trading_cost*delta_weights).sum((2,1)).reshape((-1,1))\n",
    "        mpo_objective = portfolio_returns - self.risk_aversion_coeff*portfolio_variance - portfolio_trade_cost\n",
    "        mpo_objective = mpo_objective.sum(dim=1).mean()\n",
    "        return -mpo_objective\n",
    "\n",
    "    def batch_cov(self, mu_returns):\n",
    "        B, N, D = mu_returns.size()\n",
    "        mean = mu_returns.mean(dim=1).unsqueeze(1)\n",
    "        diffs = (mu_returns - mean).reshape(B * N, D)\n",
    "        prods = torch.bmm(diffs.unsqueeze(2), diffs.unsqueeze(1)).reshape(B, N, D, D)\n",
    "        bcov = prods.sum(dim=1) / (N - 1)  # Unbiased estimate\n",
    "        return bcov  # (B, D, D)\n",
    "\n",
    "\n",
    "class SinglePeriodOptimizationObjective(nn.Module):\n",
    "    def __init__(self, holding_cost=0, trading_cost=0.01, risk_aversion_coeff=0.5):\n",
    "        super(SinglePeriodOptimizationObjective, self).__init__()\n",
    "        self.holding_cost=holding_cost\n",
    "        self.trading_cost=trading_cost\n",
    "        self.risk_aversion_coeff=risk_aversion_coeff\n",
    "    \n",
    "    def forward(self, weights_t, mean_returns):\n",
    "        weights = weights_t.squeeze(1)\n",
    "        mu_returns = mean_returns.squeeze(1)\n",
    "        portfolio_returns = (weights*mu_returns).sum(1)\n",
    "        portfolio_variance = torch.var(mean_returns, dim=2).flatten()\n",
    "        spo_objective = portfolio_returns - self.risk_aversion_coeff*portfolio_variance - self.trading_cost\n",
    "        spo_objective = spo_objective.mean()\n",
    "        return -spo_objective\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_objective_func = MultiPeriodOptimizationObjective(\n",
    "    holding_cost=0,\n",
    "    trading_cost=0.01,\n",
    "    risk_aversion_coeff=0.2)\n",
    "\n",
    "sp_objective_func = SinglePeriodOptimizationObjective(\n",
    "    holding_cost=0,\n",
    "    trading_cost=0.01,\n",
    "    risk_aversion_coeff=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_period_transformer = DeepModel(model=TransformerNet(num_features=NUM_STOCKS,\n",
    "                                                      num_assets=NUM_STOCKS,\n",
    "                                                      num_head=25,\n",
    "                                                      num_layers=10,\n",
    "                                                      W=lookback_window,\n",
    "                                                      T=predict_period),\n",
    "                                 loss_function=mp_objective_func)\n",
    "\n",
    "single_period_transformer = DeepModel(model=TransformerNet(num_features=NUM_STOCKS,\n",
    "                                                           num_assets=NUM_STOCKS,\n",
    "                                                           num_head=25,\n",
    "                                                           num_layers=10,\n",
    "                                                           W=lookback_window,\n",
    "                                                           T=1),\n",
    "                                 loss_function=sp_objective_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for X,y in X_train_iter_sp:\n",
    "    print(X.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-period Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(max_epochs=MAX_EPOCHS, logger=False, enable_checkpointing=False)\n",
    "trainer.fit(model=multi_period_transformer, train_dataloaders=X_train_iter_mp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_data = []\n",
    "for i in range(len(X_test) - lookback_window - predict_period):\n",
    "    hist_data = X_test.iloc[i:i+lookback_window]\n",
    "    pred_data = X_test.iloc[i+lookback_window:i+lookback_window+predict_period]\n",
    "    X_test_data.append([hist_data.values, pred_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_data = X_test_data[::4]\n",
    "len(X_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_period_transformer.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    result_mpo = [[multi_period_transformer(torch.FloatTensor(x)), y] for x,y in X_test_data]\n",
    "\n",
    "result_mpo = [w.numpy()*mu_returns for w,mu_returns in result_mpo]\n",
    "result_mpo = pd.concat(result_mpo)\n",
    "result_mpo = result_mpo.sum(1)\n",
    "result_mpo = np.cumsum(result_mpo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single-period Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_2 = pl.Trainer(max_epochs=MAX_EPOCHS, logger=False, enable_checkpointing=False)\n",
    "trainer_2.fit(model=single_period_transformer, train_dataloaders=X_train_iter_sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_data_sp = []\n",
    "for i in range(len(X_test) - lookback_window - 1):\n",
    "    hist_data = X_test.iloc[i:i+lookback_window]\n",
    "    pred_data = X_test.iloc[i+lookback_window:i+lookback_window+1]\n",
    "    X_test_data_sp.append([hist_data.values, pred_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    result_spo = [[single_period_transformer(torch.FloatTensor(x)), y] for x,y in X_test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_spo = [w.numpy()*mu_returns for w,mu_returns in result_spo]\n",
    "result_spo = pd.concat(result_spo)\n",
    "result_spo = result_spo.sum(1)\n",
    "result_spo = np.cumsum(result_spo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "plt.plot(result_mpo, label=\"MPO\")\n",
    "plt.plot(result_spo, label=\"SPO\")\n",
    "plt.title(\"Portfolio Weekly return\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
