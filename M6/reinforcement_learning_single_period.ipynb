{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Notebook, we will train a Reinforcement Learing Agent to maximize the cumulative return by having a portfolio that comprises of the individual stocks from the S&P100. The agent will perform the portfolio allocation and maximize its cumulative return over the training horizon. We will show at the end the out-of-sample performance. \n",
    "\n",
    "Several important criterias as follows:\n",
    "1. This is a single-period RL system. We intend to extend this to a multi-period approach by the next submission. \n",
    "2. The training dataset is between 01-01-1990 and 01-08-2017 whereas the testing dataset is between 01-09-2017 and 01-07-2024.\n",
    "3. We will use a monthly frequency instead of daily.\n",
    "4. The following features are used in this notebook: closing price, high price, low price\n",
    "5. In the RL setting:\n",
    "    - The state is a 3d tensor with shape (f,n,m). F: number of features, n: number of input periods, m: number of assets\n",
    "    - The portfolio reward is the logarithmic return of the portfolio\n",
    "    - The model used is called Ensemble of Identical Independent Evaluators: https://arxiv.org/abs/1706.10059\n",
    "    - The agent makes an action of selecting weights/allocations for these stocks\n",
    "6. For Benchmark comparison in the other Notebook, we have an Multi-Period Markowitz's Mean Variance and the S&P100 index itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger('matplotlib.font_manager').disabled = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from finrl.meta.env_portfolio_optimization.env_portfolio_optimization import PortfolioOptimizationEnv\n",
    "from finrl.agents.portfolio_optimization.models import DRLAgent\n",
    "from finrl.agents.portfolio_optimization.architectures import EIIE\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class GroupByScaler(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Sklearn-like scaler that scales considering groups of data.\n",
    "\n",
    "    In the financial setting, this scale can be used to normalize a DataFrame\n",
    "    with time series of multiple tickers. The scaler will fit and transform\n",
    "    data for each ticker independently.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, by, scaler=MaxAbsScaler, columns=None, scaler_kwargs=None):\n",
    "        \"\"\"Initializes GoupBy scaler.\n",
    "\n",
    "        Args:\n",
    "            by: Name of column that will be used to group.\n",
    "            scaler: Scikit-learn scaler class to be used.\n",
    "            columns: List of columns that will be scaled.\n",
    "            scaler_kwargs: Keyword arguments for chosen scaler.\n",
    "        \"\"\"\n",
    "        self.scalers = {}  # dictionary with scalers\n",
    "        self.by = by\n",
    "        self.scaler = scaler\n",
    "        self.columns = columns\n",
    "        self.scaler_kwargs = {} if scaler_kwargs is None else scaler_kwargs\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fits the scaler to input data.\n",
    "\n",
    "        Args:\n",
    "            X: DataFrame to fit.\n",
    "            y: Not used.\n",
    "\n",
    "        Returns:\n",
    "            Fitted GroupBy scaler.\n",
    "        \"\"\"\n",
    "        # if columns aren't specified, considered all numeric columns\n",
    "        if self.columns is None:\n",
    "            self.columns = X.select_dtypes(exclude=[\"object\"]).columns\n",
    "        # fit one scaler for each group\n",
    "        for value in X[self.by].unique():\n",
    "            X_group = X.loc[X[self.by] == value, self.columns]\n",
    "            self.scalers[value] = self.scaler(**self.scaler_kwargs).fit(X_group)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"Transforms unscaled data.\n",
    "\n",
    "        Args:\n",
    "            X: DataFrame to transform.\n",
    "            y: Not used.\n",
    "\n",
    "        Returns:\n",
    "            Transformed DataFrame.\n",
    "        \"\"\"\n",
    "        # apply scaler for each group\n",
    "        X = X.copy()\n",
    "        for value in X[self.by].unique():\n",
    "            select_mask = X[self.by] == value\n",
    "            X.loc[select_mask, self.columns] = self.scalers[value].transform(\n",
    "                X.loc[select_mask, self.columns]\n",
    "            )\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the 101 Stocks from the S&P100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_100 = [\n",
    "    'AAPL', 'ABBV', 'ABT', 'ACN', 'ADBE', 'AIG', 'AMD', 'AMGN', 'AMT', 'AMZN',\n",
    "    'AVGO', 'AXP', 'BA', 'BAC', 'BK', 'BKNG', 'BLK', 'BMY', 'BRK-B', 'C',\n",
    "    'CAT', 'CHTR', 'CL', 'CMCSA', 'COF', 'COP', 'COST', 'CRM', 'CSCO', 'CVS',\n",
    "    'CVX', 'DE', 'DHR', 'DIS', 'DUK', 'EMR', 'F', 'FDX', 'GD',\n",
    "    'GE', 'GILD', 'GM', 'GOOG', 'GOOGL', 'GS', 'HD', 'HON', 'IBM', 'INTC',\n",
    "    'INTU', 'JNJ', 'JPM', 'KO', 'LIN', 'LLY', 'LMT', 'LOW', 'MA',\n",
    "    'MCD', 'MDLZ', 'MDT', 'MET', 'META', 'MMM', 'MO', 'MRK', 'MS', 'MSFT',\n",
    "    'NEE', 'NFLX', 'NKE', 'NVDA', 'ORCL', 'PEP', 'PFE', 'PG', 'PM',\n",
    "    'QCOM', 'RTX', 'SBUX', 'SCHW', 'SO', 'SPG', 'T', 'TGT', 'TMO', 'TMUS',\n",
    "    'TSLA', 'TXN', 'UNH', 'UNP', 'UPS', 'USB', 'V', 'VZ', 'WFC', 'WMT',\n",
    "    'XOM', 'DOW', 'PYPL', 'KHC'\n",
    "]\n",
    "len(sp_100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Collection and Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_raw_df = yf.download(tickers=sp_100, start=\"1990-01-01\", end=\"2024-08-01\", interval=\"1mo\")\n",
    "portfolio_raw_df.fillna(method=\"bfill\", inplace=True)\n",
    "portfolio_raw_df = portfolio_raw_df.stack(level=1).rename_axis([\"Date\", \"Ticker\"]).reset_index(level=1)\n",
    "\n",
    "portfolio_raw_df = portfolio_raw_df.drop(\"Adj Close\", axis=1)\n",
    "portfolio_raw_df.columns.name = None\n",
    "portfolio_raw_df = portfolio_raw_df.reset_index()\n",
    "portfolio_raw_df.Date = portfolio_raw_df.Date.astype(str)\n",
    "portfolio_raw_df.columns = [\"date\", \"tic\", \"close\", \"high\", \"low\", \"open\", \"volume\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensuring the data is complete and there is no NaNs. the bar plots show they all have the same count\n",
    "plt.figure(figsize=(4,2))\n",
    "plt.bar(np.arange(0, len(sp_100), 1), portfolio_raw_df.groupby(\"tic\").count().mean(1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_norm_df = GroupByScaler(by=\"tic\", scaler=MaxAbsScaler).fit_transform(portfolio_raw_df)\n",
    "df_portfolio = portfolio_norm_df[[\"date\", \"tic\", \"close\", \"high\", \"low\", \"open\"]]\n",
    "df_portfolio_train, df_portfolio_test = train_test_split(df_portfolio, test_size=0.2, shuffle=False, random_state=43)\n",
    "len(df_portfolio_train), len(df_portfolio_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 24 # last 2 years of data\n",
    "num_features = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the Portfolio Optimization Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = PortfolioOptimizationEnv(\n",
    "        df_portfolio_train,\n",
    "        initial_amount=100000,\n",
    "        comission_fee_pct=0.0025,\n",
    "        time_window=T,\n",
    "        features=[\"close\", \"high\", \"low\"],\n",
    "        normalize_df=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set PolicyGradient parameters\n",
    "model_kwargs = {\n",
    "    \"lr\": 0.01,\n",
    "    \"policy\": EIIE,\n",
    "}\n",
    "\n",
    "# here, we can set EIIE's parameters\n",
    "policy_kwargs = {\n",
    "    \"k_size\": 3,\n",
    "    \"time_window\": T,\n",
    "}\n",
    "\n",
    "model = DRLAgent(environment).get_model(\"pg\", device, model_kwargs, policy_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training with 50 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DRLAgent.train_model(model, episodes=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.train_policy.state_dict(), \"policy_EIIE.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the test environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_test = PortfolioOptimizationEnv(\n",
    "        df_portfolio_test,\n",
    "        initial_amount=100000,\n",
    "        comission_fee_pct=0.0025,\n",
    "        time_window=T,\n",
    "        features=[\"close\", \"high\", \"low\"],\n",
    "        normalize_df=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EIIE_results = {\n",
    "    \"training\": environment._asset_memory[\"final\"],\n",
    "    \"test\": {}\n",
    "}\n",
    "\n",
    "# instantiate an architecture with the same arguments used in training\n",
    "# and load with load_state_dict.\n",
    "policy = EIIE(k_size=3, \n",
    "              time_window=T,\n",
    "              device=device)\n",
    "policy.load_state_dict(torch.load(\"policy_EIIE.pt\"))\n",
    "\n",
    "DRLAgent.DRL_validation(model, environment_test, policy=policy)\n",
    "EIIE_results[\"test\"][\"value\"] = environment_test._asset_memory[\"final\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_of_sample_df = pd.DataFrame(index=pd.to_datetime(df_portfolio_test.date.unique()[T-1:]),\n",
    "                                data=EIIE_results[\"test\"][\"value\"], columns=[\"RL\"])\n",
    "out_of_sample_metrics = out_of_sample_df.pct_change().fillna(0)\n",
    "out_of_sample_df = (out_of_sample_df/out_of_sample_df.iloc[0]) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,3))\n",
    "ax.plot(out_of_sample_df.index, out_of_sample_df, label=\"RL\")\n",
    "ax.set_ylabel('Return')\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.legend()\n",
    "plt.title(\"Portfolio Cumulative Return\")\n",
    "plt.setp(ax.get_xticklabels(), rotation=30, horizontalalignment='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing Statistics for portfolio performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import quantstats as qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_sharpe_ratio = qs.stats.sharpe(out_of_sample_metrics, periods=12).item()\n",
    "rl_mdd = qs.stats.max_drawdown(out_of_sample_metrics).item()\n",
    "rl_fapv = out_of_sample_df.iloc[-1].item()\n",
    "print(f\"RL Sharpe Ratio: {rl_sharpe_ratio}\")\n",
    "print(f\"RL Max Drawdown: {rl_mdd}\")\n",
    "print(f\"RL final Accumulated Portfolio Value (fAPV): {rl_fapv}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our finding from the Reinforcement Learning model shows that the agent was able to make more money than the benchmark but it has a way lower Sharpe ratio. This can be mitigated by better training and through the addition of more features such as technical analysis, longer lookback period and other forms of alternative data such as sentiment. We also have to train these models multiple times and average the results to have a more consistent result on model performance. Nonetheless, here we want to show that a simple RL agent can beat the benchmarks and it can be further fine-tuned and improved for better performance. In our older submission, we showed that we were able to have Deep Learning architectures that can learn better such as Transformers. In this Example, the Agent has a Policy Algorithm of Ensemble of Identical Independent Evaluators (EIIE), which is a series of convolutions trained on understanding the short-term and long term trends of these stocks. We will use other Policy Gradients such as Transformer model as our previous submission and test its predictive capability in assigning portfolio weights for optimal allocation. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
